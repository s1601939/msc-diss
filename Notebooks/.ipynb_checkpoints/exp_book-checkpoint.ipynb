{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import string\n",
    "import itertools\n",
    "from nltk.corpus import brown, movie_reviews, treebank, webtext, gutenberg\n",
    "import sense2vec\n",
    "from operator import itemgetter\n",
    "from joke_model import JokeModel\n",
    "from language_models import Sense2VecModel, Word2VecModel\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define the results array parts with \"constants\"\n",
    "class RESULT:\n",
    "    PlainText, TaggedText, TaggedWords, SimGrid, EntGrid, MinEnt, MaxEnt = range(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEBUGGING\n",
    "_VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_stopwords(fname='stopwords.txt'):\n",
    "    # stopwords =  ['a','to','of', 'so', 'on', 'the', 'into']\n",
    "    # stopwords += ['i', 'me', 'my', 'you', 'us', 'we', 'them', 'she', 'her', 'he', 'him']\n",
    "    # stopwords += ['and', 'or', 'but']\n",
    "    # stopwords += ['had', 'have', \"'ve\"]\n",
    "    # stopwords += ['is', 'are', 'am', \"'m\", 'be']\n",
    "    # stopwords += [\"'s\", \"'d\"]\n",
    "    stopWords = set(stopwords.words('english')) | {'would'}\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_stoptags(fname='stoppos.txt'):\n",
    "    allpos = ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'DET', 'INTJ', 'NOUN', 'NUM', \n",
    "            'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'NORP', \n",
    "            'FACILITY', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LANGUAGE']\n",
    "    keeppos = ['ADJ', 'ADV', 'INTJ', 'NOUN',  \n",
    "            'PROPN', 'SCONJ', 'SYM', 'VERB', 'X', 'NORP', \n",
    "            'FACILITY', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LANGUAGE']\n",
    "    stoppos = list(set(allpos) - set(keeppos))\n",
    "    return stoppos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_similarities(this_model, this_joke):\n",
    "    # probably want to make these global so it doesn't have to do this for EVERY joke\n",
    "    # or put them in the model?\n",
    "    stop_words = load_stopwords()\n",
    "    stop_tags = load_stoptags()\n",
    "\n",
    "    # remove stopwords\n",
    "    joke_words = [w for w in this_joke.split() if w.split('|')[0].lower() not in stop_words]\n",
    "    # remove unwanted tags\n",
    "    joke_words = [w for w in joke_words if w.split('|')[1] not in stop_tags]\n",
    "    # remove OOV words\n",
    "    joke_words = [w for w in joke_words if this_model.in_vocab(w)]\n",
    "\n",
    "    sim_grid = pd.DataFrame(index=joke_words, columns=joke_words)\n",
    "    #sim_grid = sim_grid.fillna(-1.0)\n",
    "\n",
    "    pairs = list(itertools.combinations(joke_words,2))\n",
    "    for (left_word,right_word) in pairs:\n",
    "        try:\n",
    "            this_sim = this_model.similarity(left_word, right_word)\n",
    "            sim_grid[left_word][right_word] = this_sim\n",
    "            sim_grid[right_word][left_word] = this_sim\n",
    "        except:\n",
    "            # we could use this to build a stopword list\n",
    "            # or we could use ContVec? to reconstruct a new vector for the OOV word?\n",
    "            print(\"one of these words is not in vocab: {0}, {1}\".format(left_word,right_word))\n",
    "    return [sim_grid, this_joke, joke_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rank_similarities(this_joke, ascending=True):\n",
    "    sim_list = sorted([(this_joke[RESULT.SimGrid][x][y],'{} {}'.format(x,y)) \n",
    "                   for xi,x in enumerate(this_joke[RESULT.TaggedWords])\n",
    "                   for yi,y in enumerate(this_joke[RESULT.TaggedWords]) if xi < yi])\n",
    "    return sim_list\n",
    "\n",
    "def meandiff_similarities(this_joke):\n",
    "    sim_grid = this_joke[RESULT.SimGrid].replace(-1,np.nan).mean()\n",
    "    sim_grid = np.abs(this_joke[RESULT.SimGrid].replace(-1,np.nan) - sim_grid.mean())\n",
    "    sim_list = sorted([(sim_grid[x][y],'{} {}'.format(x,y)) \n",
    "                       for xi,x in enumerate(this_joke[RESULT.TaggedWords])\n",
    "                       for yi,y in enumerate(this_joke[RESULT.TaggedWords]) if xi < yi])\n",
    "    return sim_list\n",
    "\n",
    "# print(results[10][0],meandiff_similarities(results[10]))\n",
    "# for r in results:\n",
    "#      print('{}\\n\\t{}\\n\\t{}\\n\\t{}\\n\\t{}'.format(r[0],\n",
    "#                                                rank_similarities(r)[0],\n",
    "#                                                rank_similarities(r)[-1],\n",
    "#                                                meandiff_similarities(r)[0],\n",
    "#                                                meandiff_similarities(r)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_similarities(this_joke,save_plot=False):\n",
    "    fig, ax1 = plt.subplots(figsize=(7,4))\n",
    "    \n",
    "    similarities = this_joke[3].replace(-1,1) # show unit similarity\n",
    "    for xi, x in enumerate(this_joke[2]):\n",
    "        for y in range(len(this_joke[2])):\n",
    "            similarities[x][y] = np.nan if xi > y else similarities[x][y]\n",
    "    heatmap = ax1.imshow(similarities, cmap='hot', interpolation='nearest')\n",
    "    ax1.set_xticks(ticks=range(len(this_joke[2])))\n",
    "    ax1.set_xticklabels(this_joke[2], rotation=60,ha='right')\n",
    "    ax1.set_yticks(ticks=range(len(this_joke[2])))\n",
    "    ax1.set_yticklabels(this_joke[2])\n",
    "\n",
    "    plt.colorbar(heatmap)\n",
    "    plt.title(\"Pairwise word similarity (cosine similarity of word vectors)\")\n",
    "    if save_plot:\n",
    "        print('not saved.')\n",
    "        pass\n",
    "        # but we need to maybe save the plots in a folder plots/joke_id\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_entropies(pos_tagged_list):\n",
    "    entropies = [model.entropy(string) for string in pos_tagged_list]\n",
    "#     print(entropies)\n",
    "#     print(np.cumsum(entropies))\n",
    "    return list(entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_entropy(this_joke,save_plot=False,show_sims=True,unit_height=False):\n",
    "    # twin scales code suggested by https://matplotlib.org/devdocs/gallery/api/two_scales.html\n",
    "    # this_joke[4] should contain the word-by-word entropy list\n",
    "    # this_joke[5] should contain the minimum entropy \"parse\"\n",
    "    # this_joke[6] should contain the maximum entropy \"parse\"\n",
    "    fig, ax1 = plt.subplots(figsize=(7,4))\n",
    "    \n",
    "    individual_entropies = this_joke[RESULT.EntGrid]\n",
    "    cumulative_entropy = list(np.cumsum(this_joke[RESULT.EntGrid]))\n",
    "    min_pos_entropy = get_entropies(this_joke[5])\n",
    "    max_pos_entropy = get_entropies(this_joke[6])\n",
    "        \n",
    "    if unit_height:\n",
    "        max_ent = cumulative_entropy[-1]\n",
    "        individual_entropies /= max_ent\n",
    "        cumulative_entropy /= max_ent\n",
    "        min_pos_entropy /= max_ent\n",
    "        max_pos_entropy /= max_ent\n",
    "        ax1.set_ylim([-0.1,1.1])\n",
    "        \n",
    "    ax1.plot(cumulative_entropy, label='cumulative')\n",
    "    ax1.plot(individual_entropies, label='jokePOS')\n",
    "    ax1.plot(min_pos_entropy, label='minPOS')\n",
    "    ax1.plot(max_pos_entropy, label='maxPOS')\n",
    "    ax1.set_ylabel('Entropy (nat)')\n",
    "\n",
    "    if show_sims:\n",
    "        # plot the similarity ranges on ax2\n",
    "        maxes = this_joke[RESULT.SimGrid].replace(-1,np.nan).max(axis=1).tolist()\n",
    "        mins = this_joke[RESULT.SimGrid].replace(-1,np.nan).min(axis=1).tolist()\n",
    "        averages = this_joke[RESULT.SimGrid].mean().tolist()\n",
    "        ax2 = ax1.twinx()\n",
    "        if unit_height:\n",
    "            ax2.set_ylim([-0.1,1.1])\n",
    "    #    ax2.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "        ax2.plot(maxes,'g^',linestyle='None')\n",
    "        ax2.plot(mins,'gv',linestyle='None')\n",
    "        ax2.plot(averages, 'kx', linestyle='dotted')\n",
    "        ax2.set_ylabel('Min/Max Similarity')\n",
    "\n",
    "\n",
    "    \n",
    "    box = ax1.get_position()\n",
    "\n",
    "    # Shrink current axis by 20%\n",
    "#     ax1.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "#    ax1.legend()\n",
    "    # Put a legend to the right of the current axis\n",
    "#     ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 1.12), ncol=4, fancybox=False, shadow=False)\n",
    "\n",
    "    ax1.set_xticks(ticks=range(len(this_joke[2])))\n",
    "    ax1.set_xticklabels(this_joke[2], rotation=23, ha='right')\n",
    "\n",
    "\n",
    "    if save_plot:\n",
    "        print('not saved.')\n",
    "        pass\n",
    "        # but we need to maybe save the plots in a folder plots/joke_id\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def main(model_choice='s2v',joke_choice='jokes.txt'):\n",
    "    print(\"Load the model\")\n",
    "    if model_choice == 'w2v':\n",
    "        print(\">>word2vec - extended corpora\")\n",
    "        model = Word2VecModel(model_choice)\n",
    "    elif model_choice == 's2v':\n",
    "        print(\">>sense2vec - reddit hivemind corpus\")\n",
    "        model = Sense2VecModel(model_choice)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    try:\n",
    "        print(\"Load the jokes\")\n",
    "        jokes = JokeModel(joke_choice,named_entities=False)\n",
    "    except:\n",
    "        raise Exception('Could not load file \"'+joke_choice+'\"')\n",
    "\n",
    "\n",
    "    results = [[j,None,None,[None]] for j in jokes.raw_jokes()]\n",
    "    joke_id = 0\n",
    "    for joke in jokes.tagged_jokes():\n",
    "        print(results[joke_id][0])\n",
    "        mns, mnw, mxs, mxw, grid, pos_joke, pos_joke_words = get_similarities(model, joke)\n",
    "        results[joke_id][1] = pos_joke\n",
    "        results[joke_id][2] = pos_joke_words\n",
    "        results[joke_id][3] = grid\n",
    "        joke_id += 1\n",
    "        \n",
    "    with open(model_choice+'_'+joke_choice+'.pkl','wb') as pkl_file:\n",
    "        pickle.dump(results, pkl_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_model(model, model_size=None, recalculate=False, write=True):\n",
    "    '''\n",
    "    model_choice: string representing a language model: Sense2VecModel, Word2VecModel\n",
    "    model_size: model-specific size string\n",
    "    from_file: string for loading instead of recalculating\n",
    "    '''\n",
    "    # These are the current known language model classes\n",
    "    model_functions = {'Sense2Vec' : Sense2VecModel,\n",
    "                       'Word2Vec'  : Word2VecModel} \n",
    "\n",
    "    # validate model_choice\n",
    "    if model not in model_functions:\n",
    "        valid_values = ', '.join([k for k,v in model_functions.items()])\n",
    "        raise ValueError(\"model value must be one of the following: '{}'\".format(valid_values))\n",
    "    \n",
    "    \n",
    "    file_name = model + ('_' + model_size if model_size is not None else '') + '.pkl'\n",
    "    if not recalculate:\n",
    "        try:\n",
    "            if _VERBOSE: print('Loading {} model from {} ...'.format(model, file_name))\n",
    "            with open(file_name,'rb') as pkl_file:\n",
    "                language_model = pickle.load(pkl_file)\n",
    "            if _VERBOSE: print('Loaded.')\n",
    "        except:\n",
    "            recalculate = True\n",
    "            \n",
    "    if recalculate:\n",
    "        if _VERBOSE: print('Building {} model ...'.format(model))\n",
    "        try:\n",
    "            language_model = model_functions[model](model, model_size)\n",
    "        except:\n",
    "            raise NotImplementedError\n",
    "        if _VERBOSE: print(\"Done.\")\n",
    "        if write:\n",
    "            with open(file_name,'wb') as pkl_file:\n",
    "                pickle.dump(language_model, pkl_file)\n",
    "            if _VERBOSE: print('results saved to {}'.format(file_name))\n",
    "\n",
    "    return language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_text(text_choice='jokes.txt'):\n",
    "    try:\n",
    "        if _VERBOSE: print(\"Loading text from {}\".format(text_choice))\n",
    "        jokes = JokeModel(text_choice,named_entities=False)\n",
    "        if _VERBOSE: print(\"Done.\")\n",
    "    except:\n",
    "        raise Exception('Could not load file \"'+text_choice+'\"')\n",
    "    return jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_text_metrics(lang_model, joke_model, recalculate=False, write=True):\n",
    "    '''\n",
    "    lang_model: Language Model\n",
    "              : selected language model to use\n",
    "    joke_model: Joke Model \n",
    "              : model_choice\n",
    "    returns   : results list\n",
    "\n",
    "    '''\n",
    "    file_name = lang_model.model_type+'_'+joke_model.joke_file+'.pkl'\n",
    "    if not recalculate:\n",
    "        try:\n",
    "            with open(file_name,'rb') as pkl_file:\n",
    "                results = pickle.load(pkl_file)\n",
    "            if _VERBOSE: print('previous results loaded from {}'.format(file_name))\n",
    "        except:\n",
    "            recalculate=True\n",
    "    \n",
    "    if recalculate:\n",
    "        results = [[j,None,None,[None],None,None,None] for j in joke_model.raw_jokes()]\n",
    "        for joke_id, joke in enumerate(joke_model.tagged_jokes()):\n",
    "            if _VERBOSE: print(results[joke_id][RESULT.PlainText])\n",
    "            grid, pos_joke, pos_joke_words = get_similarities(lang_model, joke)\n",
    "            results[joke_id][RESULT.TaggedText] = pos_joke\n",
    "            results[joke_id][RESULT.TaggedWords] = pos_joke_words\n",
    "            results[joke_id][RESULT.SimGrid] = grid\n",
    "\n",
    "            # maybe we don't store this?\n",
    "#            results[joke_id][RESULT.EntGrid] = get_entropies(pos_joke_words) # might only work for s2v\n",
    "            results[joke_id][RESULT.EntGrid] = list([lang_model.entropy(string) for string in pos_joke_words])\n",
    "\n",
    "            # should probably make this a function\n",
    "\n",
    "            permuted_tagged_sentence = [' '.join(item) for item \n",
    "                                        in list(itertools.product(*[lang_model.pos_list(w) for w in pos_joke_words]))]\n",
    "            pts_sorted = sorted([[lang_model.entropy(p),p] for p in permuted_tagged_sentence])\n",
    "            results[joke_id][5] = list([lang_model.entropy(string) for string in pts_sorted[0][1].split()]) # minimum entropy version tagged_string list\n",
    "            results[joke_id][6] = list([lang_model.entropy(string) for string in pts_sorted[-1][1].split()]) # maximum entropy version tagged_string list\n",
    "\n",
    "        if write:\n",
    "            with open(file_name,'wb') as pkl_file:\n",
    "                pickle.dump(results, pkl_file)\n",
    "            if _VERBOSE: print('results saved to {}'.format(file_name))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Sense2Vec model ...\n",
      "counting tokens\n",
      "Done.\n",
      "results saved to Sense2Vec.pkl\n"
     ]
    }
   ],
   "source": [
    "s2v_model = load_model(model='Sense2Vec', recalculate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text from jokes.txt\n",
      "Done.\n",
      "Loading text from non_jokes.txt\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "jokes = load_text('jokes.txt')\n",
    "non_jokes = load_text('non_jokes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whiskey|NOUN', 'diet|NOUN', '.|PUNCT', 'lost|VERB', 'three|QUANTITY', 'ducks|NOUN', 'already|ADV', '.|PUNCT']\n",
      "['whiskey|NOUN', 'diet|NOUN', 'lost|VERB', 'three|QUANTITY', 'ducks|NOUN', 'already|ADV']\n",
      "whiskey|NOUN\n",
      "diet|NOUN\n",
      "lost|VERB\n",
      "three|QUANTITY\n",
      "ducks|NOUN\n",
      "already|ADV\n"
     ]
    }
   ],
   "source": [
    "#print(get_similarities(s2v_model,\"i|PRON am|VERB on|ADP a|DET whiskey|NOUN diet|NOUN .|PUNCT i|PRON have|VERB lost|VERB three|QUANTITY pounds|QUANTITY already|ADV .|PUNCT\"))\n",
    "j = \"i|PRON am|VERB on|ADP a|DET whiskey|NOUN diet|NOUN .|PUNCT i|PRON have|VERB lost|VERB three|QUANTITY ducks|NOUN already|ADV .|PUNCT\"\n",
    "stop_words = load_stopwords()\n",
    "stop_tags = load_stoptags()\n",
    "this_model = s2v_model\n",
    "joke_words = [w for w in j.split() if w.split('|')[0].lower() not in stop_words]\n",
    "print(joke_words)\n",
    "joke_words = [w for w in joke_words if w.split('|')[1] not in stop_tags]\n",
    "print(joke_words)\n",
    "#joke_words = [w for w in joke_words if this_model.in_vocab(w)]\n",
    "for w in joke_words:\n",
    "    if s2v_model.in_vocab(w):\n",
    "        print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wasn't originally going to get a brain transplant, but then i changed my mind.\n",
      "i was going to get a brain transplant and then i changed my mind.\n",
      "i'd tell you a chemistry joke but i know i wouldn't get a reaction.\n",
      "i'm glad i know sign language, it's pretty handy.\n",
      "i have a few jokes about unemployed people but it doesn't matter none of them work.\n",
      "i used to be a banker, but then i lost interest.\n",
      "i hate insect puns, they really bug me.\n",
      "insect puns bug me.\n",
      "it's hard to explain puns to kleptomaniacs because they always take things literally.\n",
      "i was so sad and crying when i lost my playstation 3 but unfortunately, there was nobody to console me!\n",
      "i'm on a whiskey diet. i've lost three days already.\n",
      "she broke into song when she couldn't find the key.\n",
      "she had a boyfriend with a wooden leg, but broke it off.\n",
      "corduroy pillows are making headlines.\n",
      "if you want to catch a squirrel just climb a tree and act like a nut.\n",
      "a magician was walking down the street and turned into a grocery store.\n",
      "time flies like an arrow, fruit flies like banana.\n",
      "dwarfs and midgets have very little in common.\n",
      "my grandfather has the heart of a lion, and a lifetime ban at the zoo.\n",
      "a broken pencil is pointless.\n",
      "my skiing skills are really going downhill.\n",
      "obesity should not be taken lightly.\n",
      "being fat should not be taken lightly.\n",
      "results saved to Sense2Vec_jokes.txt.pkl\n",
      "i was going to get a brain transplant and then i decided against it.\n",
      "i'd tell you a history joke but i know i wouldn't get a reaction.\n",
      "i'm glad i know sign language, it's pretty useful.\n",
      "i have a few jokes about working people but it doesn't matter none of them work.\n",
      "i have a few jokes about unemployed people but it doesn't matter none of them are funny.\n",
      "i used to be a banker, but then i got bored.\n",
      "i used to be a farmer, but then i lost interest.\n",
      "animal puns bug me.\n",
      "insect puns annoy me.\n",
      "it's hard to explain puns to children because they always take things literally.\n",
      "i was so sad and crying when i lost my playstation 3 but unfortunately, there was nobody to comfort me!\n",
      "i'm on a whiskey diet. i've lost three pounds already.\n",
      "i'm on a whiskey diet, i've lost three kilograms already.\n",
      "she broke into the house when she couldn't find the key.\n",
      "she broke into song when she couldn't find the pitch.\n",
      "she had a stool with a wooden leg, but broke it off.\n",
      "corduroy pillows are making news.\n",
      "if you want to catch a squirrel just climb a tree and act like a lunatic.\n",
      "a magician was walking down the street and entered into a grocery store.\n",
      "a broken pencil is useless.\n",
      "my skiing skills are really failing.\n",
      "obesity should be taken seriously.\n",
      "being fat should be taken seriously.\n",
      "education should not be taken lightly.\n",
      "results saved to Sense2Vec_non_jokes.txt.pkl\n"
     ]
    }
   ],
   "source": [
    "s2v_joke_results = get_text_metrics(s2v_model,jokes,recalculate=True)\n",
    "s2v_non_joke_results = get_text_metrics(s2v_model,non_jokes,recalculate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Word2Vec model ...\n",
      "Done.\n",
      "results saved to Word2Vec_full.pkl\n"
     ]
    }
   ],
   "source": [
    "w2v_model = load_model(model='Word2Vec',model_size='full',recalculate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jokes\n",
      "i wasn't originally going to get a brain transplant, but then i changed my mind.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Word2VecModel' object has no attribute 'pos_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-4f479ada7395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jokes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_joke_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjokes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecalculate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"non-jokes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mw2v_non_joke_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnon_jokes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecalculate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-d1f4820bc48d>\u001b[0m in \u001b[0;36mget_text_metrics\u001b[0;34m(lang_model, joke_model, recalculate, write)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             permuted_tagged_sentence = [' '.join(item) for item \n\u001b[0;32m---> 35\u001b[0;31m                                         in list(itertools.product(*[lang_model.pos_list(w) for w in pos_joke_words]))]\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mpts_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpermuted_tagged_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjoke_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpts_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# minimum entropy version tagged_string list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-d1f4820bc48d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             permuted_tagged_sentence = [' '.join(item) for item \n\u001b[0;32m---> 35\u001b[0;31m                                         in list(itertools.product(*[lang_model.pos_list(w) for w in pos_joke_words]))]\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mpts_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpermuted_tagged_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjoke_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpts_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# minimum entropy version tagged_string list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecModel' object has no attribute 'pos_list'"
     ]
    }
   ],
   "source": [
    "print(\"jokes\")\n",
    "w2v_joke_results = get_text_metrics(w2v_model,jokes,recalculate=True)\n",
    "print(\"non-jokes\")\n",
    "w2v_non_joke_results = get_text_metrics(w2v_model,non_jokes,recalculate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "i|PRON am|VERB on|ADP a|DET whiskey|NOUN diet|NOUN .|PUNCT\n",
      "i|PRON have|VERB lost|VERB three|QUANTITY pounds|QUANTITY already|ADV .|PUNCT\n",
      "['whiskey|NOUN', 'diet|NOUN', 'lost|VERB', 'three|QUANTITY', 'already|ADV']\n",
      "['whiskey|NOUN', 'diet|NOUN', '.|PUNCT', 'lost|VERB', 'three|QUANTITY', 'pounds|QUANTITY', 'already|ADV', '.|PUNCT']\n",
      "['am|VERB', 'whiskey|NOUN', 'diet|NOUN', 'have|VERB', 'lost|VERB', 'three|QUANTITY', 'pounds|QUANTITY', 'already|ADV']\n",
      "['i|PRON', 'am|VERB', 'on|ADP', 'a|DET', 'whiskey|NOUN', 'diet|NOUN', '.|PUNCT', 'i|PRON', 'have|VERB', 'lost|VERB', 'three|QUANTITY', 'pounds|QUANTITY', 'already|ADV', '.|PUNCT']\n",
      "['whiskey|NOUN', 'diet|NOUN', 'lost|VERB', 'three|QUANTITY', 'pounds|QUANTITY', 'already|ADV']\n",
      "i|PRON am|VERB on|ADP a|DET whiskey|NOUN diet|NOUN .|PUNCT\n",
      "i|PRON have|VERB lost|VERB three|QUANTITY pounds|QUANTITY already|ADV .|PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.in_vocab(\"pounds|NOUN\"))\n",
    "print(w2v_non_joke_results[11][RESULT.TaggedText])\n",
    "print(w2v_non_joke_results[11][RESULT.TaggedWords])\n",
    "print([w for w in w2v_non_joke_results[11][RESULT.TaggedText].split() if w.split('|')[0].lower() not in load_stopwords()])\n",
    "print([w for w in w2v_non_joke_results[11][RESULT.TaggedText].split() if w.split('|')[1] not in load_stoptags()])\n",
    "print([w for w in w2v_non_joke_results[11][RESULT.TaggedText].split() if w2v_model.in_vocab(w.split('|')[0])])\n",
    "\n",
    "this_joke = w2v_non_joke_results[11][RESULT.TaggedText]\n",
    "# remove stopwords\n",
    "joke_words = [w for w in this_joke.split() if w.split('|')[0].lower() not in load_stopwords()]\n",
    "# remove unwanted tags\n",
    "joke_words = [w for w in joke_words if w.split('|')[1] not in load_stoptags()]\n",
    "# remove OOV words\n",
    "joke_words = [w for w in joke_words if w2v_model.in_vocab(w)]\n",
    "print(joke_words)\n",
    "\n",
    "for ji,j in enumerate(non_jokes.tagged_jokes()):\n",
    "    if ji==11: print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = 10\n",
    "b = 11\n",
    "plot_entropy(w2v_joke_results[a],save_plot=False,show_sims=True,unit_height=True)\n",
    "plot_entropy(w2v_non_joke_results[b],save_plot=False,show_sims=True,unit_height=True)\n",
    "\n",
    "# use for discovering and explaining\n",
    "plot_similarities(w2v_joke_results[a],save_plot=False)\n",
    "plot_similarities(w2v_non_joke_results[b],save_plot=False)\n",
    "\n",
    "print(w2v_joke_results[a][RESULT.PlainText])\n",
    "print(w2v_non_joke_results[b][RESULT.PlainText])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for r in s2v_joke_results:\n",
    "    plot_entropy(r,show_sims=True,save_plot=False,unit_height=True)\n",
    "    plot_similarities(r,save_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "joke_choice='non_jokes.txt'\n",
    "try:\n",
    "    print(\"Load the jokes\")\n",
    "    jokes = JokeModel(joke_choice,named_entities=False)\n",
    "    print(\"Loaded.\")\n",
    "except:\n",
    "    raise Exception('Could not load file \"'+joke_choice+'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(model_choice+'_'+joke_choice+'.pkl','rb') as pkl_file:\n",
    "        results = pickle.load(pkl_file)\n",
    "    print(\"Loaded from file\")\n",
    "except:\n",
    "    results = [[j,None,None,[None],None,None,None] for j in jokes.raw_jokes()]\n",
    "    for joke_id, joke in enumerate(jokes.tagged_jokes()):\n",
    "        print(results[joke_id][0])\n",
    "        mns, mnw, mxs, mxw, grid, pos_joke, pos_joke_words = get_similarities(model, joke)\n",
    "        results[joke_id][1] = pos_joke\n",
    "        results[joke_id][2] = pos_joke_words\n",
    "        results[joke_id][3] = grid\n",
    "\n",
    "        # maybe we don't store this?\n",
    "        results[joke_id][4] = get_entropies(pos_joke_words) # might only work for s2v\n",
    "\n",
    "        # should probably make this a function\n",
    "\n",
    "        permuted_tagged_sentence = [' '.join(item) for item \n",
    "                                    in list(itertools.product(*[model.pos_list(w) for w in pos_joke_words]))]\n",
    "        pts_sorted = sorted([[model.entropy(p),p] for p in permuted_tagged_sentence])\n",
    "        results[joke_id][5] = pts_sorted[0][1].split() # minimum entropy version tagged_string list\n",
    "        results[joke_id][6] = pts_sorted[-1][1].split() # maximum entropy version tagged_string list\n",
    "\n",
    "        with open(model_choice+'_'+joke_choice+'.pkl','wb') as pkl_file:\n",
    "            pickle.dump(results, pkl_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    plot_entropy(r,show_sims=True,save_plot=False,unit_height=True)\n",
    "    plot_similarities(r,save_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
